import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from network.MobileUViT import MobileUViT

def visualize_feature_maps():
    """å¯è§†åŒ–ç‰¹å¾å›¾çš„å˜åŒ–"""
    
    # åˆ›å»ºæ¨¡å‹å’Œè¾“å…¥
    model = MobileUViT()
    model.eval()
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥ï¼ˆåŒ»å­¦å›¾åƒé£æ ¼ï¼‰
    batch_size = 1
    input_tensor = torch.randn(batch_size, 3, 256, 256)
    
    # æ”¶é›†å„å±‚ç‰¹å¾å›¾
    feature_maps = {}
    
    def hook_fn(name):
        def hook(module, input, output):
            if isinstance(output, tuple):
                feature_maps[name] = [o.detach() for o in output]
            else:
                feature_maps[name] = output.detach()
        return hook
    
    # æ³¨å†Œhook
    hooks = []
    hooks.append(model.patch_embeddings.register_forward_hook(hook_fn('embeddings')))
    hooks.append(model.down.register_forward_hook(hook_fn('downsample')))
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = model(input_tensor)
    
    # æ¸…ç†hooks
    for hook in hooks:
        hook.remove()
    
    return model, input_tensor, feature_maps

def analyze_parameter_efficiency():
    """åˆ†æå‚æ•°æ•ˆç‡"""
    model = MobileUViT()
    
    param_analysis = {}
    total_params = 0
    
    for name, module in model.named_modules():
        if len(list(module.children())) == 0:  # å¶å­æ¨¡å—
            params = sum(p.numel() for p in module.parameters())
            if params > 0:
                param_analysis[name] = params
                total_params += params
    
    # æŒ‰æ¨¡å—ç±»å‹ç»Ÿè®¡
    module_types = {
        'Conv2d': 0,
        'Linear': 0,
        'BatchNorm2d': 0,
        'LayerNorm': 0,
        'Others': 0
    }
    
    for name, params in param_analysis.items():
        if 'Conv2d' in name:
            module_types['Conv2d'] += params
        elif 'Linear' in name:
            module_types['Linear'] += params
        elif 'BatchNorm2d' in name:
            module_types['BatchNorm2d'] += params
        elif 'LayerNorm' in name:
            module_types['LayerNorm'] += params
        else:
            module_types['Others'] += params
    
    return param_analysis, module_types, total_params

def detailed_module_analysis():
    """è¯¦ç»†æ¨¡å—åˆ†æ"""
    
    print("=== è¯¦ç»†æ¨¡å—åŠŸèƒ½åˆ†æ ===\n")
    
    print("ğŸ“Š 1. ConvUtr æ·±åº¦è§£æ:")
    print("   â”œâ”€ æ·±åº¦å¯åˆ†ç¦»å·ç§¯: 3x3åˆ†ç»„å·ç§¯ + 1x1ç‚¹å·ç§¯")
    print("   â”œâ”€ æ®‹å·®è¿æ¥: ä¿æŒæ¢¯åº¦æµåŠ¨")
    print("   â”œâ”€ GELUæ¿€æ´»: å¹³æ»‘éçº¿æ€§")
    print("   â””â”€ æ‰¹å½’ä¸€åŒ–: ç¨³å®šè®­ç»ƒ")
    
    print("\nğŸ” 2. Embeddings æ¨¡å—æ¶æ„:")
    print("   â”œâ”€ Stem: 3â†’16é€šé“çš„åŸºç¡€ç‰¹å¾æå–")
    print("   â”œâ”€ Layer1: 16â†’16é€šé“ï¼Œæ ¸å¤§å°3ï¼Œæ·±åº¦1")
    print("   â”œâ”€ Layer2: 16â†’32é€šé“ï¼Œæ ¸å¤§å°3ï¼Œæ·±åº¦1") 
    print("   â”œâ”€ Layer3: 32â†’64é€šé“ï¼Œæ ¸å¤§å°7ï¼Œæ·±åº¦3")
    print("   â””â”€ è¾“å‡º: 1/8åˆ†è¾¨ç‡çš„64é€šé“ç‰¹å¾å›¾")
    
    print("\nâš¡ 3. LGLBlock æœºåˆ¶:")
    print("   â”œâ”€ LocalAgg: 9x9å¤§æ ¸å·ç§¯è¿›è¡Œå±€éƒ¨èšåˆ")
    print("   â”œâ”€ é—¨æ§æœºåˆ¶: Sigmoidæ§åˆ¶ä¿¡æ¯æµ")
    print("   â”œâ”€ GlobalSparseAttn: ç¨€ç–å…¨å±€æ³¨æ„åŠ›")
    print("   â”œâ”€ SR-ratio=2: ç©ºé—´é™é‡‡æ ·å‡å°‘è®¡ç®—")
    print("   â””â”€ æ®‹å·®è¿æ¥: ä¿æŒåŸå§‹ä¿¡æ¯")
    
    print("\nğŸŒ 4. GlobalSparseAttn åŸç†:")
    print("   â”œâ”€ QKVæŠ•å½±: 64ç»´â†’192ç»´")
    print("   â”œâ”€ å¤šå¤´æ³¨æ„åŠ›: 8ä¸ªå¤´ï¼Œæ¯å¤´8ç»´")
    print("   â”œâ”€ ç©ºé—´é™é‡‡æ ·: 2å€å‡å°‘è®¡ç®—é‡")
    print("   â”œâ”€ è½¬ç½®å·ç§¯ä¸Šé‡‡æ ·: æ¢å¤ç©ºé—´åˆ†è¾¨ç‡")
    print("   â””â”€ LayerNorm: ç¨³å®šæ³¨æ„åŠ›è®¡ç®—")
    
    print("\nğŸ”§ 5. è§£ç å™¨è®¾è®¡:")
    print("   â”œâ”€ è·³è·ƒè¿æ¥: èåˆå¤šå°ºåº¦ç‰¹å¾")
    print("   â”œâ”€ é€æ­¥ä¸Šé‡‡æ ·: 2å€ä¸Šé‡‡æ ·Ã—3æ¬¡")
    print("   â”œâ”€ é€šé“å‹ç¼©: 64â†’32â†’16â†’1")
    print("   â””â”€ 1x1å·ç§¯: æœ€ç»ˆåˆ†å‰²é¢„æµ‹")

def memory_analysis():
    """å†…å­˜ä½¿ç”¨åˆ†æ"""
    model = MobileUViT()
    
    # åˆ†æä¸åŒè¾“å…¥å°ºå¯¸çš„å†…å­˜ä½¿ç”¨
    input_sizes = [(1, 3, 128, 128), (1, 3, 256, 256), (1, 3, 512, 512)]
    
    memory_usage = {}
    
    for size in input_sizes:
        input_tensor = torch.randn(*size)
        
        # è®¡ç®—æ¿€æ´»å†…å­˜ï¼ˆè¿‘ä¼¼ï¼‰
        def get_memory_usage():
            model.eval()
            with torch.no_grad():
                # ä½¿ç”¨hookæ”¶é›†ç‰¹å¾å›¾å¤§å°
                feature_sizes = []
                
                def hook_fn(module, input, output):
                    if isinstance(output, torch.Tensor):
                        feature_sizes.append(output.numel())
                    elif isinstance(output, tuple):
                        feature_sizes.extend([o.numel() for o in output if isinstance(o, torch.Tensor)])
                
                hooks = []
                for module in model.modules():
                    if len(list(module.children())) == 0:
                        hooks.append(module.register_forward_hook(hook_fn))
                
                _ = model(input_tensor)
                
                for hook in hooks:
                    hook.remove()
                
                return sum(feature_sizes) * 4 / (1024**2)  # MB
        
        memory_usage[f"{size[-2]}x{size[-1]}"] = get_memory_usage()
    
    return memory_usage

def create_architecture_summary():
    """åˆ›å»ºæ¶æ„æ€»ç»“"""
    
    summary = """
Mobile-UViT æ¶æ„æ€»ç»“
==================

ğŸ¯ è®¾è®¡ç›®æ ‡ï¼šç§»åŠ¨è®¾å¤‡ä¸Šçš„è½»é‡çº§åŒ»å­¦å›¾åƒåˆ†å‰²

ğŸ“ è¾“å…¥è¾“å‡ºï¼š
   - è¾“å…¥: RGBå›¾åƒ [B,3,256,256] æˆ–ç°åº¦å›¾è½¬3é€šé“
   - è¾“å‡º: åˆ†å‰²æ©ç  [B,1,256,256]

ğŸ—ï¸ æ ¸å¿ƒæ¶æ„ï¼š
   1. ç¼–ç å™¨ï¼šå¤šå°ºåº¦ç‰¹å¾æå–ï¼ˆConvUtrå—ï¼‰
   2. ç“¶é¢ˆï¼šLGLï¼ˆå±€éƒ¨-å…¨å±€-å±€éƒ¨ï¼‰æ³¨æ„åŠ›æœºåˆ¶
   3. è§£ç å™¨ï¼šè·³è·ƒè¿æ¥+é€æ­¥ä¸Šé‡‡æ ·

ğŸ’¡ åˆ›æ–°ç‚¹ï¼š
   - ConvUtrï¼šå‚æ•°é«˜æ•ˆçš„å·ç§¯æ¨¡å—
   - LGLæœºåˆ¶ï¼šå±€éƒ¨å…¨å±€ç‰¹å¾èåˆ
   - ç¨€ç–æ³¨æ„åŠ›ï¼šé™ä½è®¡ç®—å¤æ‚åº¦
   - ç§»åŠ¨ä¼˜å…ˆï¼š1.39Må‚æ•°ï¼Œé€‚åˆç§»åŠ¨è®¾å¤‡

âš™ï¸ é…ç½®å‚æ•°ï¼š
   â”œâ”€ åŸºç¡€ç‰ˆ: dims=[16,32,64,128], 1.39Må‚æ•°
   â”œâ”€ å¤§ç‰ˆ: dims=[32,64,128,256], 5.5Må‚æ•°
   â””â”€ æ·±åº¦: depths=[1,1,3,3,3]

ğŸ” å…³é”®ç‰¹æ€§ï¼š
   - æ„Ÿå—é‡ï¼šä»3x3åˆ°7x7çš„å¤§æ ¸å·ç§¯
   - æ³¨æ„åŠ›ï¼š8å¤´æ³¨æ„åŠ›ï¼Œ64ç»´åµŒå…¥
   - æ­£åˆ™åŒ–ï¼šDropPath + BatchNorm + LayerNorm
   - æ¿€æ´»ï¼šGELU + ReLUç»„åˆ
    """
    
    return summary

if __name__ == "__main__":
    # è¿è¡Œè¯¦ç»†åˆ†æ
    detailed_module_analysis()
    
    # å‚æ•°æ•ˆç‡åˆ†æ
    param_analysis, module_types, total_params = analyze_parameter_efficiency()
    
    print("\n" + "="*50)
    print("ğŸ“Š å‚æ•°æ•ˆç‡åˆ†æ")
    print("="*50)
    print(f"æ€»å‚æ•°é‡: {total_params:,}")
    print("\næŒ‰æ¨¡å—ç±»å‹åˆ†å¸ƒ:")
    for module_type, params in module_types.items():
        percentage = (params / total_params) * 100
        print(f"  {module_type}: {params:,} ({percentage:.1f}%)")
    
    # å†…å­˜ä½¿ç”¨åˆ†æ
    memory_usage = memory_analysis()
    print("\n" + "="*50)
    print("ğŸ’¾ å†…å­˜ä½¿ç”¨åˆ†æ")
    print("="*50)
    for size, memory in memory_usage.items():
        print(f"è¾“å…¥ {size}: {memory:.2f} MB")
    
    # æ‰“å°æ¶æ„æ€»ç»“
    print("\n" + create_architecture_summary())
    
    # éªŒè¯æ¨¡å‹å¯ä»¥è¿è¡Œ
    print("\n" + "="*50)
    print("âœ… æ¨¡å‹éªŒè¯")
    print("="*50)
    
    model = MobileUViT()
    model.eval()
    
    with torch.no_grad():
        # æµ‹è¯•ä¸åŒè¾“å…¥
        test_inputs = [
            torch.randn(1, 3, 256, 256),  # æ ‡å‡†è¾“å…¥
            torch.randn(1, 1, 256, 256),  # ç°åº¦å›¾è¾“å…¥
        ]
        
        for i, test_input in enumerate(test_inputs):
            if test_input.shape[1] == 1:
                print(f"ç°åº¦å›¾è¾“å…¥ {test_input.shape} -> è‡ªåŠ¨è½¬æ¢ä¸º3é€šé“")
            
            output = model(test_input)
            print(f"æµ‹è¯• {i+1}: {test_input.shape} -> {output.shape} âœ“")